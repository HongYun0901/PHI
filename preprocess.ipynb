{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "from transformers import BertTokenizer\n",
    "from random import sample\n",
    "from math import ceil\n",
    "\n",
    "type_dict = {\"none\":0, \"name\":1, \"location\":2, \"time\":3, \"contact\":4,\n",
    "             \"ID\":5, \"profession\":6, \"biomarker\":7, \"family\":8,\n",
    "             \"clinical_event\":9, \"special_skills\":10, \"unique_treatment\":11,\n",
    "             \"account\":12, \"organization\":13, \"education\":14, \"money\":15,\n",
    "             \"belonging_mark\":16, \"med_exam\":17, \"others\":18}\n",
    "\n",
    "def type2str(t):\n",
    "    for key, value in type_dict.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
    "        if t == value:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing...\n",
      "processed 120 data\n",
      "processed 522 data to length 512\n",
      "processed 120 origin datas to 345 train datas and 177 test datas in length 512\n",
      "Preprocess Done !!\n",
      "Testing set id list:  [65, 26, 106, 74, 20, 76, 72, 31, 92, 17, 115, 7, 43, 113, 19, 44, 86, 27, 85, 93, 11, 91, 63, 90, 82, 107, 51, 84, 0, 77, 35, 71, 119, 9, 98, 79, 97, 45, 10, 2]\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_LM = \"hfl/chinese-bert-wwm\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM)\n",
    "bert_data = []\n",
    "with open ('./dataset/train_1.json', 'r') as json_file:\n",
    "    data_file = json.load(json_file)\n",
    "    print(\"start preprocessing...\")\n",
    "    c = 0\n",
    "    for data in data_file:\n",
    "        article = data['article']\n",
    "        type_list = []\n",
    "        for i, item in enumerate(data['item']):\n",
    "            article = article[:item[1] + i*2] + \"_\" + item[3] + \"_\" + article[item[2] + i*2:]\n",
    "            type_list.append(type_dict[item[4]])\n",
    "        article = article.replace(\"醫師：\", \"[SEP]\") \\\n",
    "        .replace(\"民眾：\", \"[SEP]\").replace(\"家屬：\", \"[SEP]\") \\\n",
    "        .replace(\"個管師：\", \"[SEP]\").replace(\"護理師：\", \"[SEP]\")\n",
    "        tokens = tokenizer.tokenize(article)\n",
    "        \n",
    "        \n",
    "        start_pos_label = np.full(len(tokens),0)\n",
    "        end_pos_label = np.full(len(tokens),0)\n",
    "        type_label = np.full(len(tokens), 0)\n",
    "        count_back = 0\n",
    "        begin = 0\n",
    "        j = 0\n",
    "        remove_list = []\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] == '_':\n",
    "                remove_list.append(i)\n",
    "                if count_back == 0:\n",
    "                    start_pos_label[i+1] = 1\n",
    "                    begin = i\n",
    "                    count_back += 1\n",
    "                else:\n",
    "                    end_pos_label[i-1] = 1\n",
    "                    type_label[begin : i] = type_list[j]\n",
    "                    j += 1\n",
    "                    count_back = 0\n",
    "\n",
    "        start_pos_label = start_pos_label.tolist()\n",
    "        end_pos_label = end_pos_label.tolist()\n",
    "        type_label = type_label.tolist()\n",
    "\n",
    "        for i in sorted(remove_list, reverse=True):\n",
    "            del tokens[i], start_pos_label[i], end_pos_label[i] ,  type_label[i]\n",
    "        \n",
    "        # tokens[0] = \"[CLS]\"\n",
    "        if tokens[0] == \"[SEP]\":\n",
    "            del tokens[0], start_pos_label[0], end_pos_label[0] ,  type_label[0]\n",
    "        tokens.append(\"[SEP]\")\n",
    "        start_pos_label.append(0)\n",
    "        end_pos_label.append(0)\n",
    "        type_label.append(0)\n",
    "\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        pt_dict = {'input_ids':ids, \n",
    "                   \"start_pos_label\":start_pos_label, \n",
    "                   'end_pos_label' :end_pos_label,\n",
    "                   \"type_label\":type_label,\n",
    "                   'article_id':data['id']}\n",
    "        bert_data.append(pt_dict)\n",
    "        c += 1\n",
    "#         for i in range(len(start_pos_label)):\n",
    "#             if start_pos_label[i] == 1:\n",
    "#                 start = i\n",
    "#             if end_pos_label[i] == 1:\n",
    "#                 end = i\n",
    "#                 print(tokens[start:end+1] , type2str(type_label[start]))\n",
    "\n",
    "        print(\"\\rprocessed %d data\" %c, end=\"\")\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "\n",
    "\"\"\"to length 512\"\"\"\n",
    "bert_data_train_512 = []\n",
    "bert_data_test_512 = []\n",
    "c = 0\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "length = len(bert_data)\n",
    "try:\n",
    "    test_list = set(eval(sys.argv[2]))\n",
    "except:\n",
    "    test_list = sample(range(length), ceil(length * 0.33)) # split 1/3 testing data\n",
    "\n",
    "\n",
    "error_count = 0\n",
    "\n",
    "for data in bert_data:\n",
    "    c += 1\n",
    "    ids = data['input_ids']\n",
    "    start_pos_label = data['start_pos_label']\n",
    "    end_pos_label = data['end_pos_label']\n",
    "    type_ = data['type_label']\n",
    "    pos = 0\n",
    "    flag = 0\n",
    "    sep_pos = 0\n",
    "    new_pos = 0\n",
    "    while (pos < len(ids)):\n",
    "        ids_512 = ids[pos : pos + 512]\n",
    "        count_back = 0\n",
    "        for i in range(min(511, len(ids_512)-1), 0, -1):\n",
    "            if (ids_512[i] == 102): # 102 = [SEP]\n",
    "                count_back += 1\n",
    "                if (count_back == 1):\n",
    "                    sep_pos = i\n",
    "                    new_pos = pos + i + 1\n",
    "                elif (count_back <= 3): # overlap n-1 sentences \n",
    "                    new_pos = pos + i + 1\n",
    "        if(count_back == 0):\n",
    "            sep_pos = 510\n",
    "            new_pos = pos + 510 - 5 # overlap n tokens\n",
    "\n",
    "        ids_512 = [101] + ids[pos : pos+sep_pos+1] + [0] * (512 - sep_pos - 2)\n",
    "        seg_512 = [0] * 512\n",
    "\n",
    "        att_512 = [1] * (sep_pos + 2) + [0] * (512 - sep_pos - 2)\n",
    "        start_pos_label_512 = [0] + start_pos_label[pos : pos+sep_pos+1] + [0] * (512 - sep_pos - 2)\n",
    "        end_pos_label_512 = [0] + end_pos_label[pos : pos+sep_pos+1] + [0] * (512 - sep_pos - 2)\n",
    "        type_512 = [0] + type_[pos : pos+sep_pos+1] + [0] * (512 - sep_pos - 2)\n",
    "        flag = 1 if (pos+sep_pos+1 >= len(ids)) else 0\n",
    "        pos = new_pos\n",
    "        \n",
    "#         print(\"\")\n",
    "#         print('ids',len(ids_512))\n",
    "#         print('seg',len(seg_512))\n",
    "#         print('att',len(att_512))\n",
    "#         print('start',len(start_pos_label_512))\n",
    "#         print('end',len(end_pos_label_512))\n",
    "#         print('type',len(type_512))\n",
    "        \n",
    "        if len(ids_512)!= 512:\n",
    "            error_count += 1\n",
    "            continue\n",
    "\n",
    "       \n",
    "\n",
    "        pt_dict = {\"input_ids\":ids_512, \n",
    "                   \"seg\":seg_512, \n",
    "                   \"att\":att_512,\n",
    "                   \"start_pos_label\":start_pos_label_512,\n",
    "                   \"end_pos_label\":end_pos_label_512,\n",
    "                   \"type_label\":type_512,\n",
    "                   \"article_id\":data['article_id']}\n",
    "    \n",
    "        \n",
    "        if (data['article_id'] not in test_list):\n",
    "            bert_data_train_512.append(pt_dict)\n",
    "            c1 += 1\n",
    "        else:\n",
    "            bert_data_test_512.append(pt_dict)\n",
    "            c2 += 1\n",
    "        \n",
    "        print(\"\\rprocessed %d data to length 512\" %(c1+c2), end=\"\")\n",
    "\n",
    "        if (flag): # read single talk\n",
    "            break\n",
    "\n",
    "torch.save(bert_data_train_512, \"./dataset/train1_train_512_bert_data.pt\")\n",
    "torch.save(bert_data_test_512, \"./dataset/train1_test_512_bert_data.pt\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"processed %d origin datas to %d train datas and %d test datas in length 512\"\n",
    "    % (c, c1, c2))\n",
    "print(\"Preprocess Done !!\")\n",
    "print(\"Testing set id list: \", test_list)\n",
    "print(error_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
